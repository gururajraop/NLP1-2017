{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Pointwise Mutual Information\n",
    "\n",
    "This lab is about applications of _Pointwise Mutual Information_ (PMI) in natural language processing.\n",
    "\n",
    "\n",
    "### Tasks\n",
    "\n",
    "1. Find **collocations** with PMI.\n",
    "2. Create **dense vector representations** for words (also known as **word embeddings**) by taking the singular value decomposition of a PMI co-occurence matrix.\n",
    "\n",
    "### Rules\n",
    "* The lab exercises should be made in **groups of two people**.\n",
    "\n",
    "* The deadline is **Sunday 10 Dec 23:59**.\n",
    "\n",
    "* The assignment should submitted to **Blackboard** as `.ipynb`. Only **one submission per group**.\n",
    "\n",
    "* The **filename** should be `lab4_lastname1_lastname2.ipynb`, so for example `lab4_Levy_Goldberg.ipynb`.\n",
    "\n",
    "* The notebook is graded on a scale of **0-100**. The number of points for each question is indicated in parantheses. \n",
    "\n",
    "Notes on implementation:\n",
    "\n",
    "* You should **write your code and answers in this iPython Notebook** (see http://ipython.org/notebook.html for reference material). If you have problems, please contact your teaching assistant.\n",
    "\n",
    "* Use only **one cell for code** and **one cell for markdown** answers!    \n",
    "\n",
    "    * Put all code in the cell with the `# YOUR CODE HERE` comment.\n",
    "    \n",
    "    * For theoretical question, put your solution in the YOUR ANSWER HERE cell.\n",
    "    \n",
    "* Test your code and **make sure we can run your notebook**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PMI\n",
    "\n",
    "**Point-wise mutual information** is a measure of association used in information theory and statistics. In the words of [Jurafsky and Martin](https://web.stanford.edu/~jurafsky/slp3/15.pdf):\n",
    "\n",
    "> The point-wise mutual information is a measure of how often two events $x$ and $y$ occur, compared with what we would expect if they were independent.\n",
    "\n",
    "It is formally defined as follows. Let $x$ and $y$ be outcomes of discrete random variables $X$ and $Y$. Then the point-wise mutual information between $x$ and $y$ is \n",
    "\n",
    "$$PMI(x,y) = \\log\\frac{p(x,y)}{p(x)p(y)}.$$\n",
    "\n",
    "The values for $PMI$ range between minus and plus infinity. Large positive values indicate that $x$ and $y$ are strongly associated. A $PMI$ of zero means $x$ and $y$ are completely independent. And because of the logarithm, the values can also be negative, which indicates that $x$ and $y$ are co-occurring _less often_ than we would expect by chance. Negative values tend to be unreliable though, often resulting from the lack of coocurence statistics for $x$ and $y$. A more robust measure therefore is a variant of $PMI$ called the $Positive$ $PMI$:\n",
    "\n",
    "$$PPMI(x,y) = \\max(0, PMI(x, y)).$$\n",
    "\n",
    "In NLP applications, the $x$ and $y$ in the definition above are generally words, and the distributions based on their occurence in a text-corpus. Therefore from now on, let's refer to $x$ and $y$ as $w_i$ and $w_j$.\n",
    "\n",
    "----- \n",
    "\n",
    "You can read more about $PMI$ and $PPMI$ in the following sources:\n",
    "\n",
    "* [Jurafsky and Martin 15.2](https://web.stanford.edu/~jurafsky/slp3/15.pdf) is dedicated to $PMI$\n",
    "* The [Wikipedia entry on PMI](https://en.wikipedia.org/wiki/Pointwise_mutual_information) (it even has a language-related application of $PMI$)\n",
    "\n",
    "----- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Co-occurrence statistics\n",
    "\n",
    "To compute the $PMI$ of two words $w_i$ and $w_j$ we need the probabilities $p(w_i)$, $p(w_j)$ and $p(w_i,w_j)$. What are they?\n",
    "\n",
    "### Unigram\n",
    "\n",
    "The probabilites $p(w_i)$ and $p(w_j)$ are the probabilities of the words occuring by themselves in the corpus: they are the unigram probabilities of $w_i$ and $w_j$. We have seen before how to get these.\n",
    "\n",
    "### Skip-gram\n",
    "\n",
    "The probability $p(w_i, w_j)$ is the probability of $w_i$ and $w_j$ co-occuring together in the corpus. Co-occurrence is often modelled with a **skip-gram** model. A skip-gram collects the co-occurence statistics of a word with the words that surround it within some fixed **context window**, to the left of the word, and to the right of the word. \n",
    "\n",
    "Consider the following fragment of a sentence in which a context window of 3 words around the word _fox_ are shown:\n",
    "\n",
    "> ... the quick brown **fox** jumped over the ...\n",
    "\n",
    "The skipgram counts that we then extract from this fragment are \n",
    "\n",
    "$$C(quick, fox) = C(brown, fox)= C(jumped, fox) = C(over, fox) = 1,$$\n",
    "\n",
    "and \n",
    "\n",
    "$$C(the, fox) = 2.$$\n",
    "\n",
    "After all theses counts have been collected, we normalize and divide, giving probabilities $p(quick, fox), p(brown, fox)$ etc. such that\n",
    "\n",
    "$$\\sum_{\\{w,v\\}} p(w,v) = 1.$$\n",
    "\n",
    "**[Note]** The word-pairs $\\{w,v\\}$ are **unordered**. So $p(quick, fox) = p(fox, quick)$.\n",
    "\n",
    "----\n",
    "\n",
    "We have provided you below with all the code nessecary to obtain these distributions. Read this through, and pay some attention to the methods that they are provided with. These will be usefull in some of the questions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data\n",
    "\n",
    "As always, we will obtain the distributions from a large text corpus. To read in such a text we provide you with the following class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextData:\n",
    "    \"\"\"\n",
    "    Stores text data with additional attributes.\n",
    "    :param fname: a path to a txt file\n",
    "    \"\"\"\n",
    "    def __init__(self, fname):\n",
    "        self._fname = fname\n",
    "        self._data = []\n",
    "        self._w2i = defaultdict(lambda: len(self._w2i))\n",
    "        self._i2w = dict()\n",
    "        self._counter = Counter()\n",
    "        self._ntokens = 0    # number of tokens in dataset\n",
    "        \n",
    "        self._read()\n",
    "        \n",
    "    def _read(self):\n",
    "        with open(self._fname, \"r\") as fh:\n",
    "            for line in fh:\n",
    "                tokens = line.strip().lower().split()\n",
    "                self._data.append(tokens)\n",
    "                self._counter.update(tokens)\n",
    "        \n",
    "        # Store number of tokens in the text\n",
    "        self._ntokens = sum(self._counter.values())\n",
    "        \n",
    "        # Store the words in w2i in order of frequency from high to low\n",
    "        for word, _ in self._counter.most_common(self._ntokens):\n",
    "            self._i2w[self._w2i[word]] = word\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Number of tokens in the dataset\n",
    "        \"\"\"\n",
    "        return self._ntokens\n",
    "    \n",
    "    @property\n",
    "    def data(self):\n",
    "        \"\"\"\n",
    "        The data as list of lists\n",
    "        \"\"\"\n",
    "        return self._data\n",
    "    \n",
    "    @property\n",
    "    def counter(self):\n",
    "        \"\"\"\n",
    "        The word-counts as counter\n",
    "        \"\"\"\n",
    "        return self._counter\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        \"\"\"\n",
    "        Number of words in the dataset\n",
    "        \"\"\"\n",
    "        return len(self._w2i)\n",
    "    \n",
    "    @property\n",
    "    def w2i(self):\n",
    "        \"\"\"\n",
    "        Word to index dictionary\n",
    "        Words are sorted in order of frequency from high to low\n",
    "        \"\"\"\n",
    "        return self._w2i\n",
    "    \n",
    "    @property\n",
    "    def i2w(self):\n",
    "        \"\"\"\n",
    "        Inverse dictionary of w2i: index to words\n",
    "        \"\"\"\n",
    "        return self._i2w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram and Skipgram\n",
    "\n",
    "To train unigram and skipgram language models, you can use the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Unigram:\n",
    "    \"\"\"\n",
    "    A unigram language model\n",
    "    \"\"\"\n",
    "    def __init__(self, data):\n",
    "        self._data = data\n",
    "        self._unigram_counts = defaultdict(int)\n",
    "        self._unigram_probs = defaultdict(float)\n",
    "        self._unigram_counter = Counter()\n",
    "        self._unigram_distribution = Counter()\n",
    "        self._train()\n",
    "        \n",
    "    def _train(self):\n",
    "        \"\"\"\n",
    "        Trains the model trained on data\n",
    "        \"\"\"\n",
    "        # Get the word counts\n",
    "        for sent in self._data:\n",
    "            for word in sent:\n",
    "                self._unigram_counts[word] += 1\n",
    "            \n",
    "        # Normalize the word counts\n",
    "        s = sum(self._unigram_counts.values())\n",
    "        for word_pair, count in self._unigram_counts.items():\n",
    "            self._unigram_probs[word_pair] = count / s\n",
    "        \n",
    "        # There are some advantages to additionally use a Counter\n",
    "        self._unigram_counter = Counter(self._unigram_counts)\n",
    "        self._unigram_distribution = Counter(self._unigram_probs)\n",
    "        \n",
    "    def prob(self, w):\n",
    "        \"\"\"\n",
    "        Returns the unigram probability p(w)\n",
    "        \"\"\"\n",
    "        return self._unigram_probs[w]\n",
    "    \n",
    "        \n",
    "    def count(self, w):\n",
    "        \"\"\"\n",
    "        Returns the unigram count c(w)\n",
    "        \"\"\"\n",
    "        return self._unigram_counts[w]\n",
    "    \n",
    "    @property\n",
    "    def data(self):\n",
    "        \"\"\"\n",
    "        Returns the data\n",
    "        \"\"\"\n",
    "        return self._data\n",
    "\n",
    "    @property\n",
    "    def counter(self):\n",
    "        \"\"\"\n",
    "        Returns the unigram counts as Counter\n",
    "        \"\"\"\n",
    "        return self._unigram_counter\n",
    "    \n",
    "    @property\n",
    "    def distribution(self):\n",
    "        \"\"\"\n",
    "        Returns the unigram distribution as Counter\n",
    "        \"\"\"\n",
    "        return self._unigram_distribution\n",
    "    \n",
    "    \n",
    "class Skipgram:\n",
    "    \"\"\"\n",
    "    A skip-gram language model\n",
    "    Note: p(w,v) = p(v,w)\n",
    "    \"\"\"\n",
    "    def __init__(self, data, context_window=5):\n",
    "        self._data = data\n",
    "        self._context_window = context_window\n",
    "        self._skipgram_counts = defaultdict(int)\n",
    "        self._skipgram_probs = defaultdict(float)\n",
    "        self._skipgram_counter = Counter()\n",
    "        self._skipgram_distribution = Counter()\n",
    "        self._train()\n",
    "        \n",
    "    def _train(self):\n",
    "        \"\"\"\n",
    "        Trains the model\n",
    "        \"\"\"\n",
    "        # Get the co-occurrence counts\n",
    "        for sent in self._data:\n",
    "            for i in range(self._context_window, len(sent) - self._context_window):\n",
    "                context = [sent[i - j] for j in range(1, self._context_window + 1)] + \\\n",
    "                          [sent[i + j] for j in range(1, self._context_window + 1)]    \n",
    "                w = sent[i]\n",
    "                for v in context:\n",
    "                    word_pair = tuple(sorted([w, v])) # causes p(w,v) = p(v,w)\n",
    "                    self._skipgram_counts[word_pair] += 1\n",
    "            \n",
    "        # Turn the co-occurrence counts into probabilities\n",
    "        s = sum(self._skipgram_counts.values())\n",
    "        for word_pair, count in self._skipgram_counts.items():\n",
    "            self._skipgram_probs[word_pair] = count / s\n",
    "        \n",
    "        # There are some advantages to additionally use a Counter\n",
    "        self._skipgram_counter = Counter(self._skipgram_counts)\n",
    "        self._skipgram_distribution = Counter(self._skipgram_probs)\n",
    "        \n",
    "    def prob(self, w, v):\n",
    "        \"\"\"\n",
    "        Returns the skip-gram probability\n",
    "        Note: p(w,v) = p(v,w)\n",
    "        \"\"\"\n",
    "        word_pair = tuple(sorted([w,v]))\n",
    "        \n",
    "        return self._skipgram_probs[word_pair]\n",
    "    \n",
    "    def count(self, w, v):\n",
    "        \"\"\"\n",
    "        Returns the skip-gram counts count(w,v) = count(v,w)\n",
    "        \"\"\"\n",
    "        word_pair = tuple(sorted([w,v]))\n",
    "        \n",
    "        return self._skipgram_counts[word_pair]\n",
    "    \n",
    "    @property\n",
    "    def data(self):\n",
    "        \"\"\"\n",
    "        Returns the data\n",
    "        \"\"\"\n",
    "        return self._data\n",
    "    \n",
    "    @property\n",
    "    def context_window(self):\n",
    "        \"\"\"\n",
    "        Returns the context window size\n",
    "        \"\"\"\n",
    "        return self._context_window\n",
    "    \n",
    "    @property\n",
    "    def counter(self):\n",
    "        \"\"\"\n",
    "        Returns the skipgram counte as Counter,\n",
    "        so that we can use the method most_common.\n",
    "        \"\"\"\n",
    "        return self._skipgram_counter\n",
    "    \n",
    "    @property\n",
    "    def distribution(self):\n",
    "        \"\"\"\n",
    "        Returns the skipgram probs as Counter,\n",
    "        so that we can use the method most_common.\n",
    "        \"\"\"\n",
    "        return self._skipgram_distribution\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text corpora\n",
    "\n",
    "We have the following text corpora availlable for you:\n",
    "* A fragment of the _Penn Treebank_ with around 900.000 tokens\n",
    "* A collection of _TedX_ talks with around 5 million tokens\n",
    "* A collection of _Wikipedia_ entries, contained around 83 million tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Penn Treebank*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 398 ms, sys: 58.1 ms, total: 457 ms\n",
      "Wall time: 458 ms\n",
      "Number of tokens:  887521\n",
      "Vocabulary size:  9999\n"
     ]
    }
   ],
   "source": [
    "%time ptb = TextData(\"ptb.txt\")\n",
    "print(\"Number of tokens: \", len(ptb))\n",
    "print(\"Vocabulary size: \", ptb.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.9 s, sys: 160 ms, total: 7.06 s\n",
      "Wall time: 7.07 s\n",
      "CPU times: user 217 ms, sys: 1.62 ms, total: 219 ms\n",
      "Wall time: 219 ms\n"
     ]
    }
   ],
   "source": [
    "%time ptb_skipgram = Skipgram(ptb.data)\n",
    "%time ptb_unigram = Unigram(ptb.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *TedX* talks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.51 s, sys: 158 ms, total: 2.66 s\n",
      "Wall time: 2.67 s\n",
      "Number of tokens:  4466833\n",
      "Vocabulary size:  54455\n"
     ]
    }
   ],
   "source": [
    "%time ted = TextData(\"ted.txt\")\n",
    "print(\"Number of tokens: \", len(ted))\n",
    "print(\"Vocabulary size: \", ted.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32.3 s, sys: 378 ms, total: 32.6 s\n",
      "Wall time: 32.7 s\n",
      "CPU times: user 1.07 s, sys: 8.07 ms, total: 1.08 s\n",
      "Wall time: 1.08 s\n"
     ]
    }
   ],
   "source": [
    "%time ted_skipgram = Skipgram(ted.data)\n",
    "%time ted_unigram = Unigram(ted.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The collection of *Wikipedia* entries: (This will take some time. Expect around 15 mins for the skipgram model.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 20s, sys: 3min 48s, total: 5min 9s\n",
      "Wall time: 6min 58s\n",
      "Number of tokens:  83270301\n",
      "Vocabulary size:  466865\n"
     ]
    }
   ],
   "source": [
    "%time wiki = TextData(\"wiki.txt\")\n",
    "print(\"Number of tokens: \", len(wiki))\n",
    "print(\"Vocabulary size: \", wiki.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 54s, sys: 1min 2s, total: 12min 56s\n",
      "Wall time: 13min 24s\n",
      "CPU times: user 29.9 s, sys: 20.5 s, total: 50.5 s\n",
      "Wall time: 55.7 s\n"
     ]
    }
   ],
   "source": [
    "%time wiki_skipgram = Skipgram(wiki.data)\n",
    "%time wiki_unigram = Unigram(wiki.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Collocations with PPMI (40 points)\n",
    "\n",
    "$PPMI$ can be used to find [collocations](https://en.wikipedia.org/wiki/Collocation); words that co-occur significantly more often than can be atrributed to chance. $PPMI$ is a natural measure for this task: the collocations are precisely the word-pairs $w_i$ and $w_j$ for which $PPMI(w_i,w_j)$ is high!\n",
    "\n",
    "Have a look at the bottom of the [PMI wikipedia entry](https://en.wikipedia.org/wiki/Pointwise_mutual_information): there it lists a number of such collocations found with $PMI$.\n",
    "\n",
    "You will try to find collocations in the text-data that we've provided you with. First, you will need a function that computes the $PPMI$ given a $p(x)$, $p(y)$ and $p(x,y)$.\n",
    "\n",
    "**(a)** Complete the function `PPMI` below. **(20 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PPMI(unigram, skipgram, x, y):\n",
    "    \"\"\"\n",
    "    Returns the positive PMI of x and y\n",
    "    \n",
    "        PPMI(x, y) = max(0, PMI(x, y))\n",
    "        \n",
    "    where PMI(x,y) is computed using the \n",
    "    unigram and skipgram language model\n",
    "        \n",
    "        PMI(x,y) = log [p(x,y) / p(x)p(y)]\n",
    "        \n",
    "    :param unigram: an instance of Unigram\n",
    "    :param skipgram: an instance of Skipgram\n",
    "    :returns ppmi: a float between 0 and +inf\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    return ppmi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PTB\n",
    "\n",
    "**(a)** Use the function you wrote to find collocations in the *Penn Treebank* dataset. Recall: these are word-pairs in the corpus that have a high $PPMI$. Print a list of at most 30 of word-pairs (and they don't all have to make total sense!).  **(5 points)**\n",
    "\n",
    "**[Hint]** You might find that highest $PPMI$ word-pairs are not really what you are looking for. They make no sense as collocations, for example due to noise arrising from insufficient statistics. In that case you should search a little bit further down the line, with words that have a little lower $PPMI$. Note for example that the collocations in the [PMI wikipedia entry](https://en.wikipedia.org/wiki/Pointwise_mutual_information) have PMI's between 8 and 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TedX\n",
    "\n",
    "**(b)** Use the function you wrote to find collocations in the *TedX* dataset. **(5 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia\n",
    "\n",
    "**(c)** Find collocations in the *Wikipedia* dataset. **(5 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Optional]** Use the following template to query the model for PMI values for word-pairs. In particular, it is interesting to see what PMI our model assigns to the collocations in the PMI wikipedia entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extend this list as you like\n",
    "wiki_collocations = [(\"puerto\", \"rico\"), (\"los\", \"angeles\"), (\"hong\", \"kong\"), (\"carbon\", \"dioxide\"),\n",
    "                     (\"star\", \"trek\"), (\"star\", \"wars\"), (\"nobel\", \"prize\"), (\"prize\", \"laureate\"), \n",
    "                     (\"donald\", \"knuth\"), (\"discrete\", \"mathematics\"), (\"the\", \"and\"), (\"of\", \"it\")] \n",
    "\n",
    "print(\"{}{}{}\".format(\"word 1\".ljust(15), \"word 2\".ljust(15), \"PMI\"))\n",
    "print(\"-----------------------------------------------\")\n",
    "for (w, v) in wiki_collocations:\n",
    "    print(\"{}{}{}\".format(w.ljust(15), v.ljust(15), PPMI(wiki_unigram, wiki_skipgram, w, v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** Now that you have (hopefully) been succesful at finding some collocations, take a moment to compare the lists of word-pairs from above, for which $PPMI(w_i,w_j)$ is high, with the list of word-pairs for which $p(w_i,w_j)$ is high. **(5 points)**\n",
    "\n",
    "That means: \n",
    "* Print out a list of word-pairs for which $p(w_i,w_j)$ is high (the top 30 will suffice, for the corpus of your choice). \n",
    "* What difference do you see? Is the list of top $p(w_i,w_j)$ word-pairs as useful as the list of top $PPMI(w_i,w_j)$ word-pairs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Word-embeddings via PMI-SVD (60 points)\n",
    "\n",
    "Inspired by [this blog-post](http://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-word2vec/) we consider a classic method to obtain word embeddings that combines $PMI$ with linear algebra.\n",
    "\n",
    "Go ahead and read the post, it's an easy and quick read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPMI matrix\n",
    "\n",
    "**(a)** The first step is to make a PPMI matrix $P$. This matrix will have entries\n",
    "    \n",
    "$$(P)_{ij} = PPMI(w_i, w_j)$$\n",
    "\n",
    "Where $w_i$ and $w_j$ are the $i$-th and $j$-th words in our `w2i` dictionary. Finish the function `make_ppmi_matrix`. **(20 points)**\n",
    "\n",
    "**[Note]** If you really want to *scale up* (more about that below), you can consider writing a second version of the function that uses a *sparse* matrix datastructe for the $PMI$ matrix. Most of the entries in it will be zero. [Scipy sparse](https://docs.scipy.org/doc/scipy/reference/sparse.html) has a number of options. (You will not be able to plot this matrix with `imshow` though, like we will do a in a bit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_ppmi_matrix(words, unigram, skipgram):\n",
    "    \"\"\"\n",
    "    Constructs a PPMI matrix of with the words\n",
    "    in words.\n",
    "    :param words: the list of words (not indices!) for which the PPMI\n",
    "                  values will be stored in the matrix\n",
    "    :param unigram: an instance of Unigram\n",
    "    :param skipgram: an instance of Skipgram\n",
    "    :returns P: a numpy array such that P[i,j] = PPMI[words[i], words[j]]\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    return P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Use the function you wrote above to construct the $PPMI$ matrix for a corpus of your choice. **(10 points)**\n",
    "\n",
    "**[Note]** You are adviced to start with the relatively small *PTB* dataset, and a small list of words, for example only the top 1000 most frequent words. If you get this working, you can scale up to the *TedX* and *Wikipedia* datasets, and to a list of the top 5,000 or 10,000 words, or even more. (The upper limit depends on your patience, the memory availlable on your laptop, and whether you are using a sparse datastructure). But you should **already expect to get good performance in the 5,000-10,000 words range**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** Plot the $PPMI$ matrix with `plt.imshow`. You can use the following template code. What do you notice about the matrix? **(10 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ppmi_matrix = ...\n",
    "\n",
    "# The whole PPMI matrix\n",
    "ax, fig = plt.subplots(figsize=(10,10))\n",
    "plt.imshow(ppmi_matrix)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# Only the top-left corner (which hold the most frequent words)\n",
    "ax, fig = plt.subplots(figsize=(10,10))\n",
    "plt.imshow(ppmi_matrix[0:300, 0:300])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition\n",
    "\n",
    "Now that we have the $PPMI$ co-occurence matrix, we are ready to compute its **singular value decomposition (SVD)**. \n",
    "\n",
    "### Definition\n",
    "\n",
    "SVD is a very elegant linear algebra technique. It is defined as follows. Let $A \\in \\mathbb{R}^{m\\times n}$, then the singular value decomposition of A is given by\n",
    "\n",
    "$$A = U\\Sigma V^{\\top},$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $U$ is an $m \\times m$ **unitary** matrix.\n",
    "    * A real-valued matrix $U$ is called unitary when $U^{\\top}U = UU^{\\top} = I$. In other words: $U$ forms an *orthonormal* basis for $\\mathbb{R}^{m}$.\n",
    "    \n",
    "    \n",
    "* $\\Sigma$ is a **diagonal** $m \\times n$ matrix with **non-negative real numbers**.\n",
    "\n",
    "    * The diagonal values are the so called **singular values** $\\sigma_1, \\sigma_2,\\dots,\\sigma_n$ (supposing $n \\leq m$). The convention is that in the matrix $\\Sigma$ these are ordered from large to small: $\\sigma_1 \\geq \\sigma_2 \\geq \\dots \\geq \\sigma_n \\geq 0$. \n",
    "    \n",
    "    \n",
    "* $V$ is an $n \\times n$ **unitary** matrix (and $V^{\\top}$ its transpose)\n",
    "\n",
    "\n",
    "### Low-rank approximation\n",
    "\n",
    "When we select only the first $k$ collums of $U$ and $V$ and the first $k$ singular values in $\\Sigma$ we get \n",
    "* $\\tilde{U} \\in \\mathbb{R}^{m \\times k}$\n",
    "* $\\tilde{\\Sigma} \\in \\mathbb{R}^{k \\times k}$\n",
    "* $\\tilde{V} \\in \\mathbb{R}^{n \\times k}.$\n",
    "\n",
    "\n",
    "The reduced matrices can be used to make a rank $k$ matrix $\\tilde{A} \\in \\mathbb{R}^{m\\times n}$. The matrix $\\tilde{A}$ is an approximation of the matrix $A$:\n",
    "\n",
    "$$A \\approx \\tilde{U}\\tilde{\\Sigma} \\tilde{V}^{\\top} = \\tilde{A}.$$\n",
    "\n",
    "Moreover, this approximation is the 'best' approximation of $A$ in the sense that in minimizes the distance to $A$ in a type of matrix norm called the [Frobenius norm](https://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm):\n",
    "    \n",
    "$$\\tilde{A} = \\arg \\min_{A'} || A - A'||_{F}$$\n",
    "\n",
    "----- \n",
    "\n",
    "You can learn more about SVD from these sources:\n",
    "\n",
    "* [Jurafksy and Martin 16.1](https://web.stanford.edu/~jurafsky/slp3/16.pdf) (3rd edition)\n",
    "* The [Wikipedia page](https://en.wikipedia.org/wiki/Singular-value_decomposition) (with a very good visual illustration)\n",
    "* Or this long but good [blog-post series](https://jeremykun.com/2016/04/18/singular-value-decomposition-part-1-perspectives-on-linear-algebra/) (if you have some time!)\n",
    "\n",
    "----------\n",
    "\n",
    "For the purposes of this tutorial, however, all we need to know to get our embeddings is how to use SVD in python. And that is straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.linalg import svd\n",
    "\n",
    "%time U, s, Vt = svd(wiki_ppmi_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing the dimension\n",
    "\n",
    "Now we can create the rank $k$ matrix $\\tilde{U}$ by selection only the first $k$ columns of $U$. The value $k$ will be the **embedding dimension** of our word vectors. Now we have our word embeddings! They are the rows of $\\tilde{U}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 300\n",
    "U_tilde = U[:,:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look at the word embeddings we can see that, unlike the full $PMI$ matrix, the vectors in $\\tilde{U}$ are **dense**: almost all values non-zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ax, fig = plt.subplots(figsize=(15,15))\n",
    "plt.imshow(U_tilde[:800])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization with t-SNE\n",
    "\n",
    "A simple and rewarding way to evaluate to the word embeddings is by visualizing them.\n",
    "\n",
    "When visualizing the vectors, we ideally show them in their $k$-dimensional vector space, but that is impossible to do. To plot them we need to reduce their dimension much beyond $k$.\n",
    "\n",
    "A very popular method for this is [**t-SNE**](https://lvdmaaten.github.io/tsne/). t-SNE is a very effective technique for dimensionality reduction that is particularly well suited for the visualization of high-dimensional datasets. What it does is find a lower dimensional surface on which to project the high-dimensional data in such a way that the local structure of the original data is preserved as much as possible in the projected data. In simple words: data-points that are close to each other in the original space end up close to each other in the projected space. (Note: this does not hold large distances! Points that are far apart in the original space do not necessarily end up far apart in the projected space.) \n",
    "\n",
    "So, t-SNE is like Principal Component Analysis (PCA), another popular dimensionality reduction technique. But, due to the non-linear nature of the surface it finds (the 'manifold') t-SNE has more flexibility.\n",
    "\n",
    "--------\n",
    "If you want to know more about t-SNE you can read the following sources:\n",
    "* [How to Use t-SNE Effectively](https://distill.pub/2016/misread-tsne/) on Distill discusses, among others, the effects of the *perplexity* parameter in t-SNE\n",
    "* This [Scikit-learn example](http://scikit-learn.org/stable/auto_examples/manifold/plot_t_sne_perplexity.html#sphx-glr-auto-examples-manifold-plot-t-sne-perplexity-py)\n",
    "--------\n",
    "\n",
    "The code plots below uses **t-SNE** to make a two-dimensional plot with our word-embeddings. We also throw in some **K-means** clustering *in the original high-dimensional space* so that we can color our dimension-reduced word-embeddings. This gives us some for additional interpretative abilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"27b8f90f-b0fb-4bc7-9da7-aea8534306a3\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      var el = document.getElementById(\"27b8f90f-b0fb-4bc7-9da7-aea8534306a3\");\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"27b8f90f-b0fb-4bc7-9da7-aea8534306a3\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '27b8f90f-b0fb-4bc7-9da7-aea8534306a3' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.7.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.7.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.7.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.7.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "      document.getElementById(\"27b8f90f-b0fb-4bc7-9da7-aea8534306a3\").textContent = \"BokehJS is loading...\";\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.7.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.7.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.7.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.7.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.7.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.7.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"27b8f90f-b0fb-4bc7-9da7-aea8534306a3\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from bokeh.models import ColumnDataSource, LabelSet\n",
    "from bokeh.plotting import figure, show, output_file\n",
    "from bokeh.palettes import d3\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()\n",
    "\n",
    "def emb_scatter(data, names, perplexity=30.0, N=20):\n",
    "    \"\"\"\n",
    "    Uses t-SNE with given perplexity to reduce the dimension of the \n",
    "    vectors in data to 2, plots these in a bokeh 2d scatter plot, \n",
    "    and colors them with N colors using K-means clustering of the \n",
    "    originial vectors. The colored dots are tagged with labels from\n",
    "    the list names.\n",
    "    \n",
    "    :param data: numpy array of shape [num_vectors, embedding_dim]\n",
    "    :param names: a list of words of length num_vectors in the same order as data\n",
    "    :param perplexity: the perplexity for t-SNE\n",
    "    :param N: the number of clusters to find by K-means\n",
    "    \"\"\"\n",
    "    ## Try to find some clusters ##\n",
    "    print(\"Finding clusters\")\n",
    "    kmeans = KMeans(n_clusters=N)\n",
    "    kmeans.fit(data)\n",
    "    klabels = kmeans.labels_\n",
    "\n",
    "    ## Get a tsne fit ##\n",
    "    print(\"Fitting tsne\")\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity)\n",
    "    emb_tsne = tsne.fit_transform(data)\n",
    "    \n",
    "    ## Plot the tsne of the embeddings with bokeh ##\n",
    "    # source: https://github.com/oxford-cs-deepnlp-2017/practical-1\n",
    "    p = figure(tools=\"pan,wheel_zoom,reset,save\",\n",
    "               toolbar_location=\"above\",\n",
    "               title=\"T-SNE for most common words\")\n",
    "\n",
    "    # Set colormap as a list\n",
    "    colormap = d3['Category20'][N]\n",
    "    colors = [colormap[i] for i in klabels]\n",
    "\n",
    "    source = ColumnDataSource(data=dict(x1=emb_tsne[:,0],\n",
    "                                        x2=emb_tsne[:,1],\n",
    "                                        names=names,\n",
    "                                        colors=colors))\n",
    "\n",
    "    p.scatter(x=\"x1\", y=\"x2\", size=8, source=source, color='colors')\n",
    "\n",
    "    labels = LabelSet(x=\"x1\", y=\"x2\", text=\"names\", y_offset=6,\n",
    "                      text_font_size=\"8pt\", text_color=\"#555555\",\n",
    "                      source=source, text_align='center')\n",
    "    p.add_layout(labels)\n",
    "\n",
    "    show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** Use the function `emb_scatter` to plot the word-vector in $\\tilde{U}$. You are adviced to plot only the first 500-1000 word-vectors to make the resulting plot not too cluttered. **(10 points)**\n",
    "\n",
    "Now go find some interesting clusters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e)** Give a word cluster that you have found in the scatter plot that you think is particularly nice. **(10 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "**You have reached the end of this notebook!** You can read on if you are interested.\n",
    "\n",
    "Now, you can note that there are a number of hyperparameters in the model and its visualization that you can experiment with. **If you are motivated, you can experiment with these! But note that you are not required to do this.**\n",
    "\n",
    "The options are:\n",
    "* The pereplixity used in t-SNE has significant effects on the resulting visualization. Generally, values in the range 5-30 work well. If you have less data, or it is very high dimensional, you should go lower. If the data is packed closely together (lower dimension, more data), then higher perplexity generally works better.\n",
    "* The embedding dimension $k$ of the vectors. We chose $k=300$, which is relatively standard in the literature. But you also see dimension 50,100, 150, 500, even 600. Do you notice differences? And why not just even use the original dimensions of $U$? \n",
    "* The number of words you use in the $PPMI$ matrix $P$. How few can you get away with? Is more always better?\n",
    "* Use the matrix product $U\\Sigma$ for the embeddings. The paper by Levy et al. (2015) below argues for this: select the first $k$ columns of $U\\Sigma$ to give $\\tilde{U\\Sigma}$ as embeddings matrix. What this means is that each orthogonal vector $u_i$ is scaled by its singular value $\\sigma_i$ to give $\\sigma_i u_i$ before to reducing it to $\\tilde{\\sigma_i u_i}$.\n",
    "\n",
    "How to evaluate the quality of word-embeddings quantitatively? Read the following for more information on the various evaluation methods developped for this:\n",
    "* [Lecture notes](http://cs224d.stanford.edu/lecture_notes/notes2.pdf) from the Stanford course [Deep Learning for NLP](http://web.stanford.edu/class/cs224n/) \n",
    "\n",
    "The following paper compares a number the most popular word-embedding techniques with each. PPMI-SVD is one of them! Read the paper to get inspiration for your experiments to improve the above 'vanilla' implementation: \n",
    "\n",
    "* [Improving Distributional Similarity with Lessons Learned from Word Embeddings](http://www.aclweb.org/anthology/Q15-1016) (Levy et al. 2015)\n",
    "\n",
    "The following blog-post contains an interesting discussion on why low dimensional embeddings often work better than very high-dimensional ones.\n",
    "* [Word Embeddings: Explaining their properties](http://www.offconvex.org/2016/02/14/word-embeddings-2/) \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
